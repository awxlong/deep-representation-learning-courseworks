{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP0188 Coursework 2: Age Regression from Brain MRIs (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRI scans can be used to determine volumes of different types of brain tissue which are associated with age. In particular, as patients age, it is known that the ventricles enlarge (as they get filled with cerebrospinal fluid), while the volume of grey and white matter volume may decrease.\n",
    "\n",
    "Your task is to develop a deep learning model capable of predicting the (biological) age of a patient from MRI scans. Such a tool could be used in clinical practice to compare a patients 'biological' age against their 'true' chronological age. A significant discrepency in these ages might indicate the presense of a disease in the patient. \n",
    "\n",
    "You have been provided with a dataset of healthy patients. The dataset contains MRI scans of the patients, and their corresponing chronological ages (amongst other information). As the patients are healthy, we will assume that their biological and chronological ages are equal.\n",
    "\n",
    "We have provided you with helper code, and have marked additional code you will need to implement with \"ðŸš§ **Exercise** ðŸš§\". However, you are not bound to this code (i.e. you may modify it if you wish), or even this notebook - you may complete the coursework however you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Overview\n",
    "The notebook is split into 3 parts:\n",
    "\n",
    "- Part 1: Dataset analysis and defining a suitable setup (8 marks)\n",
    "- Part 2: Baseline model definition (7 marks)\n",
    "- Part 3: Improving upon the Baseline (15 marks)\n",
    "\n",
    "Please see the introduction for each section for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Dataset and Running the Notebook\n",
    "\n",
    "Use Google Colab to run the notebook. Run the cells in sequence, as per usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "* SimpleITK\n",
    "* wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SimpleITK gdown wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "from typing import List, Dict, Tuple, Literal\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display\n",
    "\n",
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory_used():\n",
    "    process = os.getpid()\n",
    "    proc = psutil.Process(process)\n",
    "    memory_info = proc.memory_info()\n",
    "    print(f\"Memory Used: {memory_info.rss / (1024**3):.2f} GB\")  # RSS (Resident Set Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for debugging the notebook locally. Leave as True when running in colab!\n",
    "download = True\n",
    "\n",
    "# Expect the download to take roughly 3 mins\n",
    "if download:\n",
    "    # Replace this with your Google Drive shared link or file ID\n",
    "    google_drive_shared_link = 'https://drive.google.com/file/d/1OzIzn9tLUmq74I6YB5_nTnvaV_R1v7kj/view?usp=sharing'\n",
    "    file_id = google_drive_shared_link.split('/')[-2]\n",
    "\n",
    "    # Construct the gdown URL\n",
    "    download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "    # Path where the downloaded file will be stored\n",
    "    output_path = 'download.zip'\n",
    "\n",
    "    # Download the file from Google Drive\n",
    "    gdown.download(download_url, output_path, quiet=False)\n",
    "\n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(output_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "\n",
    "    # Optionally, delete the zip file after extraction\n",
    "    # os.remove(output_path)\n",
    "    \n",
    "    data_dir = \"data/coursework_1_compressed/coursework_1\"\n",
    "else:\n",
    "    data_dir = \"../../coursework_1_compressed/coursework_1\" # \"../../coursework_1/coursework_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRI_TEMPLATE_FILE = os.path.join(data_dir, \"quasiraw_space-MNI152_desc-brain_T1w.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = os.path.join(data_dir, \"train\")\n",
    "TRAIN_MRI_DIR = os.path.join(TRAIN_DIR, \"quasiraw\")\n",
    "TRAIN_META_FILE = os.path.join(TRAIN_DIR, \"participants.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = os.path.join(data_dir, \"test\")\n",
    "TEST_MRI_DIR = os.path.join(TEST_DIR, \"quasiraw\")\n",
    "TEST_META_FILE = os.path.join(TEST_DIR, \"participants.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_SPACING = [2, 2, 2]\n",
    "OUT_SIZE = [96, 96, 96]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dataset analysis and define a suitable setup\n",
    "\n",
    "### Dataset analysis\n",
    "\n",
    "It is important to analyse your dataset to better understand it and to help detect any issues in the dataset. This can be done via visualizations and calculating statistics from the available information.\n",
    "\n",
    "### Defining a suitable seup\n",
    "\n",
    "Before performing any kind of model development, it is critical to define the scope of the model development process. This includes making decisions which stay fixed throughout the rest of the model development; as changing them would render model comparisons invalid. For example, comparing two models with test score calculated on different test datasets or with different metrics is meaningless.\n",
    "\n",
    "This section should help you answer following questions:\n",
    "* How should the train/validation/test set be split?\n",
    "* What metric will be used to assess model performance on the test set?\n",
    "    * It is critical to consider the broader project metric when setting this. In this case the project metric is to \"predict the patients biological age from MRI scans\". Furthermore, we want to make sure that the model can predict age well for _all_ patients, not just a subset (which may happen if the input data is skewed, for example).\n",
    "\n",
    "_Hint_: \n",
    "* Consider the following kinds of analysis:\n",
    "    * What relevant variables are available in the dataset? Do they need to be transformed?\n",
    "    * What does the target variable look like?\n",
    "    * Is the data sufficiently balanced?\n",
    "    * What is the distribution of other variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset description\n",
    "* The data provided has already been split into a training and test dataset. Both the training and test data contain: (i) a file of images in the form of numpy arrays (the MRIs); (ii) a tab seperated file called \"participants.tsv\" which contains structured data for each patient (including the overall target of interest \"age\").\n",
    "* The \"participant_id\" column defines a unique ID for each patient and can be used to link the structured data with the MRI scans. In particular, the patient with participant_id = \"100053248969\" has an associated MRI scan in the file \"sub-100053248969_preproc-quasiraw_T1w.npy\"\n",
    "\n",
    "The code below provides some helper functions for to import the relevant data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = [\"age\", \"tiv\", \"csfv\", \"gmv\", \"wmv\", \"magnetic_field_strength\", \"acquisition_setting\"]\n",
    "\n",
    "# Load the training data\n",
    "train_meta_df = pd.read_csv(TRAIN_META_FILE, delimiter='\\t', dtype=str)\n",
    "train_meta_df[num_vars] = train_meta_df[num_vars].astype(float)\n",
    "train_meta_df[\"age_round\"] = np.round(train_meta_df[\"age\"],0).astype(int)\n",
    "print(train_meta_df.shape)\n",
    "\n",
    "bins = list(range(0, 80, 10))\n",
    "bins.append(110)\n",
    "train_meta_df[\"age_10_yr_bckt_bg_70\"] = pd.cut(train_meta_df[\"age\"], bins=bins)\n",
    "print(train_meta_df[\"age_10_yr_bckt_bg_70\"].value_counts())\n",
    "train_meta_df[\"age_10_yr_bckt\"] = pd.cut(train_meta_df[\"age\"], bins=range(0, 110, 10))\n",
    "print(train_meta_df[\"age_10_yr_bckt\"].value_counts())\n",
    "\n",
    "# Also load the test data - BUT DON'T LOOK AT IT!\n",
    "test_meta_df = pd.read_csv(TEST_META_FILE, delimiter='\\t', dtype=str)\n",
    "test_meta_df[num_vars] = test_meta_df[num_vars].astype(float)\n",
    "test_meta_df[\"age_round\"] = np.round(train_meta_df[\"age\"],0).astype(int)\n",
    "print(test_meta_df.shape)\n",
    "\n",
    "test_meta_df[\"age_10_yr_bckt_bg_70\"] = pd.cut(test_meta_df[\"age\"], bins=bins)\n",
    "print(test_meta_df[\"age_10_yr_bckt_bg_70\"].value_counts())\n",
    "test_meta_df[\"age_10_yr_bckt\"] = pd.cut(test_meta_df[\"age\"], bins=range(0, 110, 10))\n",
    "print(test_meta_df[\"age_10_yr_bckt\"].value_counts())\n",
    "\n",
    "train_meta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptions for relevant (non-self-descriptive) column names:\n",
    "- **participant_id:** Unique ID for each patient, can be used to link to the MRI scans.\n",
    "- **csfv:** Cerebrospinalfluid volume\n",
    "- **gmv:** Grey matter volume\n",
    "- **wmv:** White matter volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patient_mri_array(\n",
    "    patient_id:str,\n",
    "    mri_dir:str\n",
    "    ) -> np.array:\n",
    "    \"\"\"Function to load a patient .nii.gz file containing an MRI 3D scan into a\n",
    "    numpy array\n",
    "    \n",
    "    Args:\n",
    "        patient_id (str): Patient ID string\n",
    "        mri_dir (str): String representing the directory containing the MRI\n",
    "        .nii.gz files\n",
    "        \n",
    "    Returns:\n",
    "        np.array: 3D numpy array representing the MRI scan\n",
    "    \"\"\"\n",
    "    mri_file = f\"nifti/sub-{patient_id}_preproc-quasiraw_T1w.nii.gz\"\n",
    "    img_array = sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(mri_dir, mri_file)))\n",
    "    return np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "def vis_raw_mri_image(\n",
    "    img_array:np.array, \n",
    "    x:int=None, \n",
    "    y:int=None, \n",
    "    z:int=None, \n",
    "    crosshair:bool=False, \n",
    "    template_file:str=MRI_TEMPLATE_FILE\n",
    "):\n",
    "    \"\"\"Function to display orthogonal 2D slices of the 3D MRI image\n",
    "\n",
    "    Args:\n",
    "        img_array (np.array): 3D numpy array representing the MRI scan\n",
    "        x (int, optional): x slice co-ordinate. Defaults to None.\n",
    "        y (int, optional): y slice co-ordinate. Defaults to None.\n",
    "        z (int, optional): z slice co-ordinate. Defaults to None.\n",
    "        crosshair (bool, optional): Flag that determines whether the images \n",
    "        should be shown with x/y lines across the centres of the axes. \n",
    "        Defaults to False.\n",
    "        template_file (str, optional): String representing the MRI template file\n",
    "        to extract the image dimensions. Defaults to MRI_TEMPLATE_FILE.\n",
    "    \"\"\"\n",
    "    template = sitk.ReadImage(template_file)\n",
    "    size = template.GetSize()\n",
    "    spacing = template.GetSpacing()\n",
    "    width  = size[0] * spacing[0]\n",
    "    height = size[1] * spacing[1]\n",
    "    depth  = size[2] * spacing[2]\n",
    "    \n",
    "    if x is None:\n",
    "        x = np.floor(size[0]/2).astype(int)\n",
    "    if y is None:\n",
    "        y = np.floor(size[1]/2).astype(int)\n",
    "    if z is None:\n",
    "        z = np.floor(size[2]/2).astype(int)\n",
    "    \n",
    "    # Display the orthogonal slices    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))\n",
    "    \n",
    "    ax1.imshow(img_array[z,:,:], extent=(0, width, height, 0))\n",
    "    ax2.imshow(img_array[:,y,:], origin='lower', extent=(0, width,  0, depth))\n",
    "    ax3.imshow(img_array[:,:,x], origin='lower', extent=(0, height, 0, depth))\n",
    "\n",
    "    # Additionally display crosshairs\n",
    "    if crosshair:\n",
    "        ax1.axhline(y * spacing[1], lw=1)\n",
    "        ax1.axvline(x * spacing[0], lw=1)\n",
    "        ax2.axhline(z * spacing[2], lw=1)\n",
    "        ax2.axvline(x * spacing[0], lw=1)\n",
    "        ax3.axhline(z * spacing[2], lw=1)\n",
    "        ax3.axvline(y * spacing[1], lw=1)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the helper functions above to visualise the MRI scans for patient \"100053248969\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = load_patient_mri_array(\"100053248969\", mri_dir=TRAIN_MRI_DIR)\n",
    "vis_raw_mri_image(img_array.squeeze())\n",
    "display(img_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸš§ **Exercise 1.1** ðŸš§\n",
    "\n",
    "Below, perform any data analysis / data visualizations you think will be useful. These may help you better understand the distribution and demographics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Insert code here #\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸš§ **Exercise 1.2** ðŸš§\n",
    "\n",
    "In the code block below, add: \n",
    "* ```train_prop```: The proportion of the train data that you will assign to training the model (where the remaining will be used for validation)\n",
    "* ```stratification_variables```: Whether you intend to stratify by any variables when creating the train/validation split\n",
    "* ```test_metric```: The metric you intend to use to assess performance on the test set\n",
    "\n",
    "(Stratified sampling is a technique used to ensure that the subsets of the data (in this case, training, validation, and test sets) are representative of the whole dataset. This is especially important in cases where the dataset is not homogeneous and contains distinct groups that should be evenly represented in each set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: input answer here\n",
    "# Your code here\n",
    "train_prop = \n",
    "stratification_variables = # a list of variables from the meta data, e.g. [\"variable_1\", \"variable_2\"]. Ensure you choose at least one variable, or the code will not run!\n",
    "test_metric =  # This is not used later in the notebook, but it is good to decide it now\n",
    "# Your code here - END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines three numpy arrays ```train_pats```, ```val_pats``` and ```test_pats``` containing a (potentially stratified) random selection of patient ids assigned to the respective datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_df = train_meta_df.groupby(by=stratification_variables)\n",
    "train_pats = []\n",
    "val_pats = []\n",
    "test_pats = []\n",
    "for idx, grp in grp_df:\n",
    "    train, val = np.split(\n",
    "        grp[\"participant_id\"], \n",
    "        [\n",
    "            int(np.floor(grp.shape[0]*train_prop))\n",
    "        ]\n",
    "    )\n",
    "    train_pats.append(train) \n",
    "    val_pats.append(val)\n",
    "train_pats = np.concatenate(train_pats)\n",
    "val_pats = np.concatenate(val_pats)\n",
    "test_pats = test_meta_df[\"participant_id\"].values\n",
    "\n",
    "with open(os.path.join(TRAIN_DIR, \"train_pats.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(train_pats, f)\n",
    "\n",
    "with open(os.path.join(TRAIN_DIR, \"val_pats.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(val_pats, f)\n",
    "\n",
    "with open(os.path.join(TEST_DIR, \"test_pats.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(test_pats, f)\n",
    "\n",
    "print(f\"Num training patients: {len(train_pats)}\")\n",
    "print(f\"Num validation patients: {len(val_pats)}\")\n",
    "print(f\"Num testing patients: {len(test_pats)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-ZwwDl-E88D"
   },
   "source": [
    "## Part 2: Baseline model definition\n",
    "\n",
    "Now the model development assumptions have been defined, a baseline model needs to be developed. This baseline model should act as a spring board for your subsequent model development. Ideally should balance the following objectives:\n",
    "* Attain a __reasonable__ level of performance, and;\n",
    "* Be __simple__! This is almost the most important requirement since if the model is too complex, understanding which areas of the archtecture are underperforming will be challenging!\n",
    "\n",
    "\n",
    "In this exercise the baseline model should be defined by the following pipeline:\n",
    "\n",
    "1. **Volume prediction:** A deep learning model is trained to take in patient MRI images and predict three patient brain volume values (explained in more detail below).\n",
    "2. **MLP Regression:** A multi-layer perceptron regression model takes patient brain volume values as input and predicts patient age.\n",
    "\n",
    "These models will be trained seperately, then combined to make end-to-end MRI -> age predictions at test time. We elaborate more on the motivation behind this pipeline below.\n",
    "\n",
    "__Your main task__ will be to define the deep learning Volume Prediction model and subsequently justify and explain your design choices. \n",
    "\n",
    "_Hints_:\n",
    "* What might be a 'simple' go-to deep learning architecture for processing images?\n",
    "* Do 3D images need to be treated differently to 2D images (see Pytorch [CONV3D](https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#conv3d)...)\n",
    "* How might 'reasonable' be defined?\n",
    "    * Does the baseline perform better than random chance?\n",
    "    * Does the baseline perform better than just applying a linear regression or xgBoost model directly on the images?\n",
    "* As this is a baseline, performance is not expected to be excellent. As you are only expected to implement the volume predictor, it is ok if other parts of the pipline are limiting. However, you may make small modifications to the mlp training, etc. to ensure reasonable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9udIgNvFE88F"
   },
   "source": [
    "### Data Helpers\n",
    "\n",
    "Before we begin, lets load and process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_mean_unit_var(\n",
    "    img_array:np.array\n",
    "    )->np.array:\n",
    "    \"\"\"Function to normalise an input image to have 0 mean and unit variance\n",
    "\n",
    "    Args:\n",
    "        img_array (np.array): 3D numpy array representing the MRI scan\n",
    "\n",
    "    Returns:\n",
    "        np.array: Normalised version of img_array\n",
    "    \"\"\"\n",
    "    mean = np.mean(img_array)\n",
    "    std = np.std(img_array)\n",
    "    # Capture 0 values as these are background\n",
    "    zero_values = img_array == 0\n",
    "    if std > 0:\n",
    "        img_array = (img_array - mean) / std\n",
    "        img_array[zero_values] = 0\n",
    "    return img_array\n",
    "\n",
    "\n",
    "def resample_image(\n",
    "    img_array:np.array, \n",
    "    out_spacing:Tuple[float]=(1.0, 1.0, 1.0), \n",
    "    out_size:Tuple[float]=None, \n",
    "    is_label:bool=False, \n",
    "    pad_value=0\n",
    "    )->np.array:\n",
    "    \"\"\"Function to alter the proportions of an input image represented as a \n",
    "    numpy array  \n",
    "\n",
    "    Args:\n",
    "        img_array (np.array): 3D numpy array representing the MRI scan\n",
    "        out_spacing (Tuple[float], optional): ???. Defaults to (1.0, 1.0, 1.0).\n",
    "        out_size (Tuple[float], optional): Tuple of length 3 defining the \n",
    "        desired output dimensions of the image. Defaults to None.\n",
    "        is_label (bool, optional): ???. Defaults to False.\n",
    "        pad_value (int, optional): ???. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        np.array: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    image = sitk.GetImageFromArray(img_array)\n",
    "    original_spacing = np.array(image.GetSpacing())\n",
    "    original_size = np.array(image.GetSize())\n",
    "\n",
    "    if out_size is None:\n",
    "        out_size = np.round(np.array(original_size * original_spacing / np.array(out_spacing))).astype(int)\n",
    "    else:\n",
    "        out_size = np.array(out_size)\n",
    "\n",
    "    original_direction = np.array(image.GetDirection()).reshape(len(original_spacing),-1)\n",
    "    original_center = (np.array(original_size, dtype=float) - 1.0) / 2.0 * original_spacing\n",
    "    out_center = (np.array(out_size, dtype=float) - 1.0) / 2.0 * np.array(out_spacing)\n",
    "\n",
    "    original_center = np.matmul(original_direction, original_center)\n",
    "    out_center = np.matmul(original_direction, out_center)\n",
    "    out_origin = np.array(image.GetOrigin()) + (original_center - out_center)\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetOutputSpacing(out_spacing)\n",
    "    resample.SetSize(out_size.tolist())\n",
    "    resample.SetOutputDirection(image.GetDirection())\n",
    "    resample.SetOutputOrigin(out_origin.tolist())\n",
    "    resample.SetTransform(sitk.Transform())\n",
    "    resample.SetDefaultPixelValue(pad_value)\n",
    "\n",
    "    if is_label:\n",
    "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "    else:\n",
    "        resample.SetInterpolator(sitk.sitkBSpline)\n",
    "\n",
    "    return sitk.GetArrayFromImage(resample.Execute(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create seperate dataloaders for the volume prediction task and the linear regression task.\n",
    "\n",
    "First, we create the volume prediction data loaders. This data loader provides a batch of 3D MRI images as 'X', and the corresponding brain volume labels as 'y'. The train/val/test splits you decided previously will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumePredictionDataset(Dataset):\n",
    "    '''\n",
    "    Important: By default, this dataset returns normalized versions of the input MRI images and output volume labels.\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        patient_list:List[str], \n",
    "        mri_file_dir:str,\n",
    "        meta_df:pd.DataFrame,\n",
    "        volume_norm_stats:Dict[str,np.array],\n",
    "        out_spacing = OUT_SPACING, \n",
    "        out_size = OUT_SIZE,\n",
    "    ):\n",
    "        self.samples:List[Dict[str,torch.tensor]] = []\n",
    "        \n",
    "        for pat in tqdm(patient_list, desc='Loading Data'):\n",
    "            \n",
    "            # MRI image\n",
    "            X = load_patient_mri_array(pat, mri_dir=mri_file_dir).squeeze()\n",
    "            X = zero_mean_unit_var(X)\n",
    "            if (out_spacing is not None) and (out_size is not None):\n",
    "                X = resample_image(X, out_spacing=out_spacing, out_size=out_size)\n",
    "            X = torch.from_numpy(X).unsqueeze(0).float()\n",
    "            \n",
    "            # Volume labels\n",
    "            y = meta_df[meta_df[\"participant_id\"] == pat][[\"csfv\", \"gmv\", \"wmv\"]].values.squeeze()\n",
    "            y = (y-volume_norm_stats['mean'])/volume_norm_stats['std']\n",
    "            y = torch.from_numpy(y).float()\n",
    "            \n",
    "            sample = {'X': X,  \"y\":y}\n",
    "            self.samples.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.samples[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data normalization:**\n",
    "\n",
    "In our datasets, we will normalize both the input and the output data for model training. Images are simply normalized to each have zero mean and unit variance. Volumes and ages are normalized using statistics of the train dataset, calculated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization stats\n",
    "\n",
    "age_norm_stats = {\n",
    "    'mean': train_meta_df[\"age\"].mean(axis=0),\n",
    "    'std': train_meta_df[\"age\"].std(axis=0)\n",
    "}\n",
    "\n",
    "volume_norm_stats = {\n",
    "    'mean': train_meta_df[[\"csfv\", \"gmv\", \"wmv\"]].mean(axis=0).values,\n",
    "    'std': train_meta_df[[\"csfv\", \"gmv\", \"wmv\"]].std(axis=0).values\n",
    "}\n",
    "\n",
    "display(age_norm_stats)\n",
    "display(volume_norm_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expect this cell to take 8 mins\n",
    "\n",
    "batch_size = 8 # Can be changed if you wish\n",
    "\n",
    "vol_train_data = VolumePredictionDataset(\n",
    "    patient_list=train_pats, \n",
    "    mri_file_dir=TRAIN_MRI_DIR,\n",
    "    meta_df=train_meta_df,\n",
    "    volume_norm_stats=volume_norm_stats,\n",
    ")\n",
    "\n",
    "vol_train_loader = DataLoader(vol_train_data, batch_size=batch_size)\n",
    "\n",
    "vol_val_data = VolumePredictionDataset(\n",
    "    patient_list=val_pats, \n",
    "    mri_file_dir=TRAIN_MRI_DIR,\n",
    "    meta_df=train_meta_df,\n",
    "    volume_norm_stats=volume_norm_stats,\n",
    ")\n",
    "\n",
    "vol_val_loader = DataLoader(vol_val_data, batch_size=batch_size)\n",
    "\n",
    "vol_test_data = VolumePredictionDataset(\n",
    "    patient_list=test_pats, \n",
    "    mri_file_dir=TEST_MRI_DIR,\n",
    "    meta_df=test_meta_df,\n",
    "    volume_norm_stats=volume_norm_stats,\n",
    ")\n",
    "\n",
    "tmp = next(vol_train_loader.__iter__())\n",
    "print(f\"Dataloader has output type {type(tmp)} with keys {tmp.keys()}\")\n",
    "print(f\"The input (MRI images) dimensions are: {tmp['X'].shape}\")\n",
    "print(f\"The output (csfv, gmv, wmv brain volumes) dimensions are: {tmp['y'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you understand what the inputs and outputs represent, and what each axis of the data represents.\n",
    "\n",
    "Next, we define the data loaders for the MLP age regression task. These return brain volumes as 'X' and age labels as 'y'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeRegressionDataset(Dataset):\n",
    "    '''\n",
    "    Important:\n",
    "    - By default, this dataset returns normalized versions of the input volumes and output age labels.\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        patient_list:List[str],\n",
    "        meta_df:pd.DataFrame,\n",
    "        volume_norm_stats:Dict[str,np.array],\n",
    "        age_norm_stats:Dict[str,np.array],\n",
    "    ):  \n",
    "        self.samples:List[Dict[str,torch.tensor]] = []\n",
    "        \n",
    "        for pat in tqdm(patient_list, desc='Loading Data'):\n",
    "            \n",
    "            # Brain volumes\n",
    "            X = meta_df[meta_df[\"participant_id\"] == pat][[\"csfv\", \"gmv\", \"wmv\"]].values.squeeze()\n",
    "            X = (X-volume_norm_stats['mean'])/volume_norm_stats['std']\n",
    "            X = torch.from_numpy(X).float()\n",
    "            \n",
    "            # Ages\n",
    "            y = meta_df[meta_df[\"participant_id\"] == pat][[\"age\"]].values.squeeze()\n",
    "            y = (y-age_norm_stats['mean'])/age_norm_stats['std']\n",
    "            y = torch.tensor(y).float()\n",
    "            \n",
    "            sample = {'X': X,  \"y\":y}\n",
    "            self.samples.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.samples[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_seed = 42\n",
    "batch_size = 2\n",
    "\n",
    "mlp_train_data = AgeRegressionDataset(\n",
    "    patient_list=train_pats,\n",
    "    meta_df=train_meta_df,\n",
    "    volume_norm_stats=volume_norm_stats,\n",
    "    age_norm_stats=age_norm_stats,\n",
    ")\n",
    "mlp_train_loader = DataLoader(mlp_train_data, batch_size=batch_size)\n",
    "\n",
    "mlp_val_data = AgeRegressionDataset(\n",
    "    patient_list=val_pats,\n",
    "    meta_df=train_meta_df,\n",
    "    volume_norm_stats=volume_norm_stats,\n",
    "    age_norm_stats=age_norm_stats,\n",
    ")\n",
    "mlp_val_loader = DataLoader(mlp_val_data, batch_size=batch_size)\n",
    "\n",
    "tmp = next(mlp_train_loader.__iter__())\n",
    "print(f\"Dataloader has output type {type(tmp)} with keys {tmp.keys()}\")\n",
    "print(f\"The input (csfv, gmv, wmv brain volumes) dimensions are: {tmp['X'].shape}\")\n",
    "print(f\"The output (patient age) dimensions are: {tmp['y'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, ensure you understand what the inputs and outputs represent, and what each axis of the data represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivating the baseline model pipeline\n",
    "\n",
    "Our training dataset provides ground-truth information about patient brain volumes, namely:\n",
    "- csfv: Cerebrospinal fluid volume\n",
    "- gmv: Grey matter volume\n",
    "- wmv: White matter volume\n",
    "\n",
    "It has been found previously that these brain volumes are correlated with age. Lets investigate if that is the case in our dataset. If so, these values could be useful for our task of predicting brain age!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ages\n",
    "ages = train_meta_df[\"age\"].values\n",
    "\n",
    "# Get volumes\n",
    "volumes = train_meta_df[[\"csfv\", \"gmv\", \"wmv\"]].values\n",
    "csfv = volumes[:,0]\n",
    "gmv = volumes[:,1]\n",
    "wmv = volumes[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots to visualize the correlation between volumes and age\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure and a 1x3 subplot (for 3 plots in a row)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "# Plot csfv vs age\n",
    "axes[0].scatter(ages, csfv, color='r', alpha=0.6)\n",
    "axes[0].set_title('CSFV vs Age')\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('CSFV')\n",
    "\n",
    "# Plot gmv vs age\n",
    "axes[1].scatter(ages, gmv, color='g', alpha=0.6)\n",
    "axes[1].set_title('GMV vs Age')\n",
    "axes[1].set_xlabel('Age')\n",
    "axes[1].set_ylabel('GMV')\n",
    "\n",
    "# Plot wmv vs age\n",
    "axes[2].scatter(ages, wmv, color='b', alpha=0.6)\n",
    "axes[2].set_title('WMV vs Age')\n",
    "axes[2].set_xlabel('Age')\n",
    "axes[2].set_ylabel('WMV')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some visual evidence here that age is correlated with the brain volumes. This means the brain volume information may be useful for predicting age. \n",
    "\n",
    "Thus, we may be able to split our baseline model into two parts: (i) Volume Prediction Model: a deep learning model that predicts brain volumes from MRI images; and (ii) MLP Regression Model: a multi-layer perceptron regression model that predicts age from brain volumes.\n",
    "\n",
    "Note, this approach leverages the brain volume information available to in the training data, but will not require any ground-truth brain volume information at deplyoment! (At training, the Linear Regression Model will be trained to map from ground-truth volumes to age. But at deployment it will take volume values predicted by the Volume Prediction Model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: MLP Regression training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets train the MLP regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define some standard training and testing functions.\n",
    "\n",
    "(**Optional Exercise**: You may modify these training and testing functions if you wish to add more sophisticated logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for step, data in enumerate(dataloader):\n",
    "        X, y = data['X'].to(device), data['y'].to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    print(f\"Train loss: {average_loss:>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, y_norm_stats):\n",
    "    def denorm(y):\n",
    "        return y.cpu() * y_norm_stats['std'] + y_norm_stats['mean']\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss, mae_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            X, y = data['X'].to(device), data['y'].to(device)\n",
    "            pred = model(X)\n",
    "            \n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            mae_loss += torch.mean(torch.abs(denorm(pred) - denorm(y))).item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    mae_loss /= num_batches\n",
    "    print(f\"Test loss: {test_loss:>8f}, MAE: {mae_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the linear regression model in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegression(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(MLPRegression, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "tmp = next(mlp_train_loader.__iter__())\n",
    "in_shape = tmp['X'].shape[1:]\n",
    "\n",
    "mlp_model = MLPRegression(in_shape[0]).float().to(device)\n",
    "\n",
    "output = mlp_model(tmp['X'].to(device))\n",
    "print(\"MLP Regression Model output shape: \", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the lmlp regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_loss_fn = nn.MSELoss()\n",
    "mlp_optimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(mlp_train_loader, mlp_model, mlp_loss_fn, mlp_optimizer)\n",
    "    test(mlp_val_loader, mlp_model, mlp_loss_fn, age_norm_stats)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all has gone well, you should see the loss decreasing on both the train and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check some predictions to visually inspect the models accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ages_list = []\n",
    "predicted_ages_list = []\n",
    "\n",
    "# Loop through the loader for a certain number of batches (e.g., 10 batches in this example)\n",
    "for i, batch in enumerate(mlp_train_loader):\n",
    "    with torch.no_grad():\n",
    "        inputs = batch['X'].to(device)\n",
    "        true_ages = batch['y'].cpu().numpy().reshape(-1)\n",
    "        predicted_ages = mlp_model(inputs).detach().cpu().numpy().reshape(-1)\n",
    "    true_ages_list.extend(true_ages)\n",
    "    predicted_ages_list.extend(predicted_ages)\n",
    "\n",
    "# denormalize the true ages\n",
    "true_ages_list = np.array(true_ages_list) * age_norm_stats['std'] + age_norm_stats['mean']\n",
    "predicted_ages_list = np.array(predicted_ages_list) * age_norm_stats['std'] + age_norm_stats['mean']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(true_ages_list, predicted_ages_list, alpha=0.6)\n",
    "plt.title('True Age vs Predicted Age for Several Batches')\n",
    "plt.xlabel('True Age')\n",
    "plt.ylabel('Predicted Age')\n",
    "plt.plot([min(true_ages_list), max(true_ages_list)], [min(true_ages_list), max(true_ages_list)], color='red')  # Line of best fit\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Deep Neural Network Brain Volume Prediction\n",
    "\n",
    "Now you must define and train your volume prediction model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸš§ **Exercise 2.1** ðŸš§\n",
    "\n",
    "Define the volume prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolumePredictor(nn.Module):\n",
    "    def __init__(self, in_shape, out_size):\n",
    "        super(VolumePredictor, self).__init__()\n",
    "        ##################\n",
    "        # Your code here #\n",
    "        ##################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        ##################\n",
    "        # Your code here #\n",
    "        ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and run a batch through the model to check it works\n",
    "tmp = next(vol_train_loader.__iter__())\n",
    "in_shape = tmp['X'].shape[1:]\n",
    "out_size = tmp['y'].shape[-1]\n",
    "\n",
    "vol_model = VolumePredictor(in_shape, out_size).float().to(device)\n",
    "\n",
    "input = tmp['X'].to(device)\n",
    "output = vol_model(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train volume prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional Exercise:** Feel free to modify the training loop, training parameters, etc, if there is anything you wish to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_loss_fn = nn.MSELoss()\n",
    "vol_optimizer = torch.optim.Adam(vol_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(vol_train_loader, vol_model, vol_loss_fn, vol_optimizer)\n",
    "    test(vol_val_loader, vol_model, vol_loss_fn, volume_norm_stats)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the volume predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_csfv_list = []\n",
    "predicted_csfv_list = []\n",
    "\n",
    "true_gmv_list = []\n",
    "predicted_gmv_list = []\n",
    "\n",
    "true_wmv_list = []\n",
    "predicted_wmv_list = []\n",
    "\n",
    "# Loop through the train loader ðŸš§ Exercise: Is the train data the best dataset to use to visualize model performance here? ðŸš§\n",
    "for batch in vol_train_loader:\n",
    "    with torch.no_grad():\n",
    "        inputs = batch['X'].to(device)\n",
    "        true_volumes = batch['y'].cpu().numpy()\n",
    "        \n",
    "        # Predict volumes\n",
    "        predicted_volumes = vol_model(inputs).detach().cpu().numpy()\n",
    "        \n",
    "        # Denormalize predicted volumes and true volumes\n",
    "        predicted_volumes = predicted_volumes * volume_norm_stats['std'] + volume_norm_stats['mean']\n",
    "        true_volumes = true_volumes * volume_norm_stats['std'] + volume_norm_stats['mean']\n",
    "        \n",
    "        # Append true and predicted volumes to the respective lists\n",
    "        true_csfv_list.extend(true_volumes[:, 0])\n",
    "        predicted_csfv_list.extend(predicted_volumes[:, 0])\n",
    "        \n",
    "        true_gmv_list.extend(true_volumes[:, 1])\n",
    "        predicted_gmv_list.extend(predicted_volumes[:, 1])\n",
    "        \n",
    "        true_wmv_list.extend(true_volumes[:, 2])\n",
    "        predicted_wmv_list.extend(predicted_volumes[:, 2])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "true_csfv_array = np.array(true_csfv_list)\n",
    "predicted_csfv_array = np.array(predicted_csfv_list)\n",
    "\n",
    "true_gmv_array = np.array(true_gmv_list)\n",
    "predicted_gmv_array = np.array(predicted_gmv_list)\n",
    "\n",
    "true_wmv_array = np.array(true_wmv_list)\n",
    "predicted_wmv_array = np.array(predicted_wmv_list)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "# Plot true vs predicted volumes for csfv\n",
    "axes[0].scatter(true_csfv_array, predicted_csfv_array, alpha=0.6)\n",
    "axes[0].plot([min(true_csfv_array), max(true_csfv_array)], [min(true_csfv_array), max(true_csfv_array)], color='red')\n",
    "axes[0].set_title('True CSFV vs Predicted CSFV')\n",
    "axes[0].set_xlabel('True CSFV')\n",
    "axes[0].set_ylabel('Predicted CSFV')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot true vs predicted volumes for gmv\n",
    "axes[1].scatter(true_gmv_array, predicted_gmv_array, alpha=0.6)\n",
    "axes[1].plot([min(true_gmv_array), max(true_gmv_array)], [min(true_gmv_array), max(true_gmv_array)], color='red')\n",
    "axes[1].set_title('True GMV vs Predicted GMV')\n",
    "axes[1].set_xlabel('True GMV')\n",
    "axes[1].set_ylabel('Predicted GMV')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Plot true vs predicted volumes for wmv\n",
    "axes[2].scatter(true_wmv_array, predicted_wmv_array, alpha=0.6)\n",
    "axes[2].plot([min(true_wmv_array), max(true_wmv_array)], [min(true_wmv_array), max(true_wmv_array)], color='red')\n",
    "axes[2].set_title('True WMV vs Predicted WMV')\n",
    "axes[2].set_xlabel('True WMV')\n",
    "axes[2].set_ylabel('Predicted WMV')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Combine models to attain end-to-end predictions\n",
    "\n",
    "Finally, we will combine the deep learning volume prediction model with the MLP regression model to obtain end-to-end predictions of age from MRI images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the end-to-end validation dataset and data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndToEndDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        patient_list:List[str], \n",
    "        mri_file_dir:str,\n",
    "        meta_df:pd.DataFrame,\n",
    "        ages_norm_stats:Dict[str,np.array],\n",
    "        out_spacing = OUT_SPACING, \n",
    "        out_size = OUT_SIZE,\n",
    "    ):\n",
    "        self.samples:List[Dict[str,torch.tensor]] = []\n",
    "        \n",
    "        for pat in tqdm(patient_list, desc='Loading Data'):\n",
    "            \n",
    "            # MRI images\n",
    "            X = load_patient_mri_array(pat, mri_dir=mri_file_dir).squeeze()\n",
    "            X = zero_mean_unit_var(X)\n",
    "            if (out_spacing is not None) and (out_size is not None):\n",
    "                X = resample_image(X, out_spacing=out_spacing, out_size=out_size)\n",
    "            X = torch.from_numpy(X).unsqueeze(0).float()\n",
    "            \n",
    "            # Ages\n",
    "            y = meta_df[meta_df[\"participant_id\"] == pat][[\"age\"]].values.squeeze()\n",
    "            y = (y-ages_norm_stats['mean'])/ages_norm_stats['std']\n",
    "            y = torch.tensor(y).unsqueeze(0).float()\n",
    "            \n",
    "            sample = {'X': X,  \"y\":y}\n",
    "            self.samples.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.samples[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_val_data = EndToEndDataset(\n",
    "    patient_list=val_pats, \n",
    "    mri_file_dir=TRAIN_MRI_DIR,\n",
    "    meta_df=train_meta_df,\n",
    "    ages_norm_stats=age_norm_stats,\n",
    ")\n",
    "\n",
    "e2e_val_loader = DataLoader(e2e_val_data, batch_size=2)\n",
    "\n",
    "tmp = next(e2e_val_loader.__iter__())\n",
    "print(f\"Dataloader has output type {type(tmp)} with keys {tmp.keys()}\")\n",
    "print(f\"The input dimensions are: {tmp['X'].shape}\")\n",
    "print(f\"The output dimensions are: {tmp['y'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the combined model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, volume_predictor, age_regressor):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.volume_predictor = volume_predictor\n",
    "        self.age_regressor = age_regressor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.volume_predictor(x)\n",
    "        return self.age_regressor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model = CombinedModel(vol_model, mlp_model).float().to(device)\n",
    "\n",
    "tmp = next(e2e_val_loader.__iter__())\n",
    "output = combined_model(tmp['X'].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess performance of end-to-end model on the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_loss_fn = nn.MSELoss()\n",
    "test(e2e_val_loader, combined_model, e2e_loss_fn, age_norm_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ages_list = []\n",
    "predicted_ages_list = []\n",
    "\n",
    "# Loop through the loader for a certain number of batches of the validation data (e.g., 10 batches in this example)\n",
    "for i, batch in enumerate(e2e_val_loader):\n",
    "    with torch.no_grad():\n",
    "        inputs = batch['X'].to(device)\n",
    "        true_ages = batch['y'].cpu().numpy().reshape(-1)\n",
    "        predicted_ages = combined_model(inputs).detach().cpu().numpy().reshape(-1)\n",
    "    true_ages_list.extend(true_ages)\n",
    "    predicted_ages_list.extend(predicted_ages)\n",
    "\n",
    "# denormalize the true ages\n",
    "true_ages_list = np.array(true_ages_list) * age_norm_stats['std'] + age_norm_stats['mean']\n",
    "predicted_ages_list = np.array(predicted_ages_list) * age_norm_stats['std'] + age_norm_stats['mean']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(true_ages_list, predicted_ages_list, alpha=0.6)\n",
    "plt.title('True Age vs Predicted Age')\n",
    "plt.xlabel('True Age')\n",
    "plt.ylabel('Predicted Age')\n",
    "plt.plot([min(true_ages_list), max(true_ages_list)], [min(true_ages_list), max(true_ages_list)], color='red')  # Line of best fit\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸš§ **Exercise 2.2** ðŸš§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse you results and consider writting about the following in your report:\n",
    "\n",
    "- Justification for your volume prediction model structure/design\n",
    "- Discussion of results from volume prediction model\n",
    "- Discussion of results from the MLP regression model\n",
    "- Discusion of results from end-to-end combined model\n",
    "- Discussion of reasonability of performance (and what the limitations may be and why)\n",
    "\n",
    "You should provide evidence to backup your discussions and any conclusions. This may include showing performance on held out validation/test sets (using the metrics you chose previously), and other statistical tests or visualisations. You do not have to limit yourself to the visualisations/analysis already implemented in the notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Insert report here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBZQKFv9E88N"
   },
   "source": [
    "## Part 3: Improving upon the Baseline\n",
    "\n",
    "Here you should make three meaningful attempts to improve upon the baseline model.\n",
    "\n",
    "Start by analysing the performance of the baseline model and propose a hypothesis for where the model could be improved. The hypothesis could align to one of:\n",
    "* Architecture: Would a different NN architecture be better? Maybe try training a deep learning model end to end, rather than first predicting volumes then predicting age.\n",
    "* Hyperparameters: Is the learning rate set correctly? Should early stopping or other kinds of regularizations be used?\n",
    "* Auxiliary losses: Can you use the extra information in the training data to provide richer training signals to the model?\n",
    "* Data augmentation: Can simple augmentations improve performance? (This can be especially helpful when the dataset is small!)\n",
    "* Skewed dataset: Are there techniques that can be used to account for the negative effects of a skewed/imbalanced dataset?\n",
    "\n",
    "Now implement a new model based on your hypothesis!\n",
    "\n",
    "Iterate though this procedure until you have proposed 3 hypotheses and developed 3 models.\n",
    "\n",
    "Donâ€™t worry if an experiment does not produce the intended results - write about why you think it didnâ€™t produce those results! Note, that since this is deep learning, there may not always be an obvious explanation - in these cases so long as your initial hypothesis was valid and you have made attempts to find an explanation (where possible) you will not lose marks.\n",
    "\n",
    "If you feel a set of experiments are leading you down a dead end - donâ€™t worry! Write about why you feel that line of enquiry is not working, take a few steps back (even if that means going back to the baseline model) and start again for your next hypothesis. Failed experiments often yield interesting and insightful results!\n",
    "\n",
    "**Further guidelines for the hypotheses:**\n",
    "\n",
    "* Scope: Ensure your hypothesis is not too limited in scope. For example, simply changing the learning rate value once would be insufficient. Instead, you could try a sweep over learning rates.\n",
    "* Grounded reasoning: Your reasoning and justifications should be grounded in what you have learned in the course materials/lectures/tutorials. \n",
    "* Evidence-backed conclusions: If you are making conclusions, ensure to present suitably strong evidence. If there is not enough evidence to make a strong conclusion, ensure you acknowledge this.\n",
    "\n",
    "**Marking:**\n",
    "\n",
    "In the report, marks will primarily be awarded for:\n",
    "- The quality of your hypotheses, and their justifications (including how you move from one hypothesis/experiment to the next)\n",
    "- The quality/thoroughness of the experiments you run to test your hypotheses, and your presentation and discussion of the results\n",
    "\n",
    "You will __not__ be marked on the overall performance of your model. This coursework is designed to test your ability to propose reasonable experiments and to test your understanding of the content of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš§ Exercise 3.1: Hypothesis 1 ðŸš§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When detailing your Hypothesis 1-3 in the report, some points to touch on include:\n",
    "\n",
    "- Explain your hypothesis and the reasoning behind it (e.g. why do you think this could improve performance, how does it relate to your previous experiment).\n",
    "- How do you intend to test the hypothesis (i.e. what experiments will you run)?\n",
    "- What evidence is required to confirm/disprove the hypothesis?\n",
    "- What do you hope to learn from your experiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Test Hypothesis 1: INSERT CODE HERE\n",
    "#\n",
    "# Feel free to use as many cells as necessary\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When discussing the results of your hypothesis 1-3 experiments, some points to touch on include:\n",
    "\n",
    "- Are the results as expected?\n",
    "- How strong are the conclusions you can draw, based on the evidence you have collected?\n",
    "- Any interesting findings or potential interesting followup experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš§ Exercise 3.2: Hypothesis 2 ðŸš§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Test Hypothesis 2: INSERT CODE HERE\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš§ Exercise 3.3: Hypothesis 3 ðŸš§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Test Hypothesis 3: INSERT CODE HERE\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš§ Exercise 3.4: Concluding Discussion ðŸš§\n",
    "\n",
    "Now you should have trained and validated 4 models (the baseline, and your 3 hypotheses). Below, you should do a final comparison of performances of the models **using the test data!**. This should be the **first and only time** you use the test data!\n",
    "\n",
    "In your report, you should discuss potential reasons for differences in performance between all 4 models, and conclude which model is best (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# INSERT CODE HERE\n",
    "################################################"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "CwGN7GmyE88E",
    "q4u-uIlPE88G",
    "Zbt8GlpeE88M",
    "7X-nVVCNE88M",
    "uOw3BNOIE88M"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TA_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
